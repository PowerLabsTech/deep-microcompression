{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4388a3",
   "metadata": {},
   "source": [
    "# TFLite vs. DMC Comparison Reproduction Guide\n",
    "\n",
    "This guide explains how to reproduce the TFLite vs. DMC comparison (Table 2) from the \"Deep Microcompression\" paper.\n",
    "\n",
    "## Required File Structure\n",
    "\n",
    "This script assumes it is located within the original project's directory structure under the experiments directory. The development module must be accessible two levels up.\n",
    "\n",
    "\n",
    "## What to Expect\n",
    "\n",
    "The script will run the full comparison, which involves:\n",
    "\n",
    "1. Baseline TF Model: Trains the TensorFlow/Keras LeNet-5 model (25 epochs) and saves it as lenet5_model.keras. This is the single source for both TFLite and DMC models.\n",
    "\n",
    "1. Comparison Stage 1 (Float32): Compares the accuracy and size of the TFLite Float32 model vs. the DMC Float32 model.\n",
    "\n",
    "1. Comparison Stage 2 (Dynamic Quantization): Compares TFLite Dynamic Quantization vs. DMC Dynamic Quantization.\n",
    "\n",
    "1. Comparison Stage 3 (Static Quantization): Compares TFLite Static INT8 Quantization vs. DMC Static INT8 Quantization.\n",
    "\n",
    "The script will print the Accuracy and Model Size for all six models, finishing with a summary table that directly reproduces the data for Table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f4f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Force TensorFlow to CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076300d",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb088eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/Documents/EmbeddedAI/deep-microcompression/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-01 20:10:55.930630: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764616255.952061   20626 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764616255.958548   20626 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764616255.974864   20626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764616255.974886   20626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764616255.974889   20626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764616255.974890   20626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271c082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes the script is in 'project_root/experiments/reproduce_table'\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "try:\n",
    "    from development import (\n",
    "        Sequential, Conv2d, Linear, ReLU, BatchNorm2d,\n",
    "        MaxPool2d, Flatten, QuantizationScheme, QuantizationGranularity\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"Error: Could not import the 'development' module.\")\n",
    "    print(\"Please ensure this script is run from 'experiments/lenet5/'\")\n",
    "    print(\"and the 'development' module is in the project root ('../../../').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf0c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "DEVICE = \"cpu\"  # Force PyTorch to CPU for fair comparison\n",
    "\n",
    "LUCKY_NUMBER = 25\n",
    "LENET5_TF_FILE = \"lenet5_model.keras\"\n",
    "INPUT_SHAPE_TORCH = (1, 28, 28)\n",
    "INPUT_SHAPE_TF = (28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454779e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(LUCKY_NUMBER)\n",
    "tf.random.set_seed(LUCKY_NUMBER)\n",
    "np.random.seed(LUCKY_NUMBER)\n",
    "random.seed(LUCKY_NUMBER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ddc6a",
   "metadata": {},
   "source": [
    "### Getting the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb8fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "def get_data_loaders():\n",
    "    \"\"\"Loads MNIST data for both TF/Numpy and PyTorch.\"\"\"\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    (mnist_train_image, mnist_train_label), (mnist_test_image, mnist_test_label) = mnist.load_data()\n",
    "    \n",
    "    # TF/Numpy data (normalized, reshaped)\n",
    "    np_train_img = (mnist_train_image / 255.0).astype(np.float32).reshape(-1, 28, 28, 1)\n",
    "    np_train_lbl = mnist_train_label.astype(np.int64)\n",
    "    np_test_img = (mnist_test_image / 255.0).astype(np.float32).reshape(-1, 28, 28, 1)\n",
    "    np_test_lbl = mnist_test_label.astype(np.int64)\n",
    "\n",
    "    # PyTorch data (needs channel-first)\n",
    "    torch_train_img = torch.from_numpy(np_train_img.transpose(0, 3, 1, 2))\n",
    "    torch_train_lbl = torch.from_numpy(np_train_lbl)\n",
    "    torch_test_img = torch.from_numpy(np_test_img.transpose(0, 3, 1, 2))\n",
    "    torch_test_lbl = torch.from_numpy(np_test_lbl)\n",
    "\n",
    "    torch_train_dataset = data.TensorDataset(torch_train_img, torch_train_lbl)\n",
    "    torch_test_dataset = data.TensorDataset(torch_test_img, torch_test_lbl)\n",
    "    \n",
    "    torch_train_loader = data.DataLoader(torch_train_dataset, batch_size=32, shuffle=True)\n",
    "    torch_test_loader = data.DataLoader(torch_test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return (np_train_img, np_train_lbl), (np_test_img, np_test_lbl), torch_train_loader, torch_test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249f31b",
   "metadata": {},
   "source": [
    "### Defining and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30952277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. TF Model ---\n",
    "def get_tf_model(train_data, test_data):\n",
    "    \"\"\"Trains or loads the baseline TF/Keras model.\"\"\"\n",
    "    print(\"\\n--- STAGE 1: Training/Loading TF Baseline Model ---\")\n",
    "    (train_img, train_lbl) = train_data\n",
    "    (test_img, test_lbl) = test_data\n",
    "    \n",
    "    if os.path.exists(LENET5_TF_FILE):\n",
    "        print(f\"Loading existing TF model from {LENET5_TF_FILE}...\")\n",
    "        return tf.keras.models.load_model(LENET5_TF_FILE)\n",
    "    \n",
    "    print(\"No TF model found. Training from scratch (up to 25 epochs)...\")\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=INPUT_SHAPE_TF),\n",
    "        tf.keras.layers.ZeroPadding2D(padding=2),\n",
    "        tf.keras.layers.Conv2D(filters=6, kernel_size=5, padding=\"valid\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=5, padding=\"valid\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=84),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Dense(units=10)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        train_img, train_lbl,\n",
    "        epochs=25,\n",
    "        batch_size=32, \n",
    "        validation_data=(test_img, test_lbl),\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving TF model to {LENET5_TF_FILE}...\")\n",
    "    model.save(LENET5_TF_FILE)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50312b45",
   "metadata": {},
   "source": [
    "### Helper function for comparing TFLite and DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a74f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TF to DMC (PyTorch) Converter ---\n",
    "@torch.no_grad()\n",
    "def copy_tensor(tensor_source, tensor_destination):\n",
    "    tensor_destination.copy_(tensor_source)\n",
    "\n",
    "@torch.no_grad()\n",
    "def convert_tf_to_dmc(tf_model):\n",
    "    \"\"\"\n",
    "    Converts the trained Keras model to the DMC Sequential model.\n",
    "    It implements layer conversion for just layers needed for vgg13.\n",
    "    \"\"\"\n",
    "    print(\"Converting TF model to DMC (PyTorch) model...\")\n",
    "    prev_layer_is_flatten = False\n",
    "    last_conv_channel = None\n",
    "    dmc_layers = []\n",
    "    pad_next_conv = None\n",
    "\n",
    "    for layer in tf_model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            pass\n",
    "        elif isinstance(layer, tf.keras.layers.ZeroPadding2D):\n",
    "            pad_next_conv = layer.padding\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            last_conv_channel = layer.weights[0].shape[-1]\n",
    "            weight_np = np.transpose(layer.weights[0].numpy(), (3, 2, 0, 1)) # TF(kH,kW,in,out) -> Torch(out,in,kH,kW)\n",
    "            out_channels, in_channels, kernel_size, _ = weight_np.shape\n",
    "            stride = layer.strides[0]\n",
    "            padding_str = layer.padding\n",
    "            pad = [0]*4 if padding_str == \"valid\" else [(kernel_size - 1)//2]*4\n",
    "            if pad_next_conv is not None:\n",
    "                for i, padding in enumerate(pad_next_conv):\n",
    "                    pad[i*2] = padding[0]\n",
    "                    pad[i*2 + 1] = padding[1]\n",
    "                pad_next_conv = None\n",
    "            has_bias = len(layer.weights) > 1\n",
    "            conv_layer = Conv2d(in_channels, out_channels, kernel_size, stride, pad=pad, bias=has_bias)\n",
    "            if has_bias:\n",
    "                copy_tensor(torch.from_numpy(layer.weights[1].numpy()), conv_layer.bias)\n",
    "            copy_tensor(torch.from_numpy(weight_np), conv_layer.weight)\n",
    "            dmc_layers.append(conv_layer)\n",
    "\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            weight_np = layer.weights[0].numpy() # TF(in, out)\n",
    "            if prev_layer_is_flatten:\n",
    "                # Handle TF's channel-last flatten for Dense layer\n",
    "                weight_np = weight_np.reshape(-1, last_conv_channel, weight_np.shape[-1])\n",
    "                weight_np = np.transpose(weight_np, (1, 0, 2))\n",
    "                weight_np = weight_np.reshape(-1, weight_np.shape[-1])\n",
    "                prev_layer_is_flatten = False\n",
    "            \n",
    "            weight_np = np.transpose(weight_np, (1, 0)) # TF(in,out) -> Torch(out,in)\n",
    "            out_features, in_features = weight_np.shape\n",
    "            \n",
    "            linear_layer = Linear(out_features=out_features, in_features=in_features, bias=True)\n",
    "            copy_tensor(torch.from_numpy(layer.weights[1].numpy()), linear_layer.bias)\n",
    "            copy_tensor(torch.from_numpy(weight_np), linear_layer.weight)\n",
    "            dmc_layers.append(linear_layer)         \n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            bn_layer = BatchNorm2d(num_features=layer.gamma.shape[0], affine=layer.scale or layer.center, eps=layer.epsilon, momentum=(1 - layer.momentum))\n",
    "            copy_tensor(torch.from_numpy(layer.gamma.numpy()), bn_layer.weight)\n",
    "            copy_tensor(torch.from_numpy(layer.beta.numpy()), bn_layer.bias)\n",
    "            copy_tensor(torch.from_numpy(layer.moving_mean.numpy()), bn_layer.running_mean)\n",
    "            copy_tensor(torch.from_numpy(layer.moving_variance.numpy()), bn_layer.running_var)\n",
    "            dmc_layers.append(bn_layer)\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.ReLU):\n",
    "            dmc_layers.append(ReLU())\n",
    "        elif isinstance(layer, tf.keras.layers.Flatten):\n",
    "            prev_layer_is_flatten = True\n",
    "            dmc_layers.append(Flatten())\n",
    "        elif isinstance(layer, tf.keras.layers.MaxPool2D):\n",
    "            stride = layer.strides[0]\n",
    "            kernel_size = layer.pool_size[0]\n",
    "            dmc_layers.append(MaxPool2d(kernel_size=kernel_size, stride=stride))\n",
    "        else: \n",
    "            raise RuntimeError(f\"Unknown layer type: {type(layer)}\")\n",
    "        \n",
    "    return Sequential(*dmc_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2552beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. TFLite Helper Functions ---\n",
    "def convert_tf_to_tflite(tf_model, scheme=QuantizationScheme.NONE, test_data=None):\n",
    "    \"\"\"Converts Keras model to TFLite flatbuffer.\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\n",
    "    \n",
    "    if scheme == QuantizationScheme.DYNAMIC:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    if scheme == QuantizationScheme.STATIC:        \n",
    "        (train_img, _) = test_data\n",
    "        def representative_dataset():\n",
    "            for i in range(100): # Use 100 batches for calibration\n",
    "                yield [train_img[i*32:(i+1)*32]]\n",
    "                \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "\n",
    "    return converter.convert()\n",
    "\n",
    "def get_tflite_model_accuracy(tflite_model, test_data, scheme=QuantizationScheme.NONE):\n",
    "    \"\"\"Evaluates a TFLite flatbuffer model.\"\"\"\n",
    "    (_, test_lbl) = test_data\n",
    "    (test_img, _) = test_data\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    tflite_predicted = []\n",
    "    \n",
    "    for image in tqdm(test_img, desc=\"Evaluating TFLite Model\"):\n",
    "        if scheme == QuantizationScheme.STATIC:\n",
    "            scale, zero_point = input_details[\"quantization\"]\n",
    "            image = ((image / scale) + zero_point).astype(np.int8)\n",
    "        \n",
    "        interpreter.set_tensor(input_details[\"index\"], image.reshape(1, 28, 28, 1))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[\"index\"])\n",
    "        tflite_predicted.append(np.argmax(output_data))\n",
    "\n",
    "    tflite_predicted = np.array(tflite_predicted)\n",
    "    return (tflite_predicted == test_lbl).sum() / len(test_lbl)\n",
    "\n",
    "# --- 5. PyTorch (DMC) Accuracy Helper ---\n",
    "def accuracy_fun(y_pred, y_true):\n",
    "    return (y_pred.argmax(dim=1) == y_true).to(torch.float).sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1744c197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "\n",
      "--- STAGE 1: Training/Loading TF Baseline Model ---\n",
      "Loading existing TF model from lenet5_model.keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 20:10:59.049580: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting TF model to DMC (PyTorch) model...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# --- Load Data ---\n",
    "tf_train_data, tf_test_data, torch_train_loader, torch_test_loader = get_data_loaders()\n",
    "\n",
    "# --- Get Baseline Models ---\n",
    "tf_model = get_tf_model(tf_train_data, tf_test_data)\n",
    "dmc_base_model = convert_tf_to_dmc(tf_model).to(DEVICE)\n",
    "dmc_metrics = {\"acc\": accuracy_fun}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37cb35a",
   "metadata": {},
   "source": [
    "### NO QUANTIZATION (Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3821e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2: Running Float32 (No Quantization) Comparison ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpt5w5fn3c/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt5w5fn3c/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpt5w5fn3c'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127492804330896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804333584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804332432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804333968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804331664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804333392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804335504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804336080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804336464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804337424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804337232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804336656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804335312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804338000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804338768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804337616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1764616260.179319   20626 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1764616260.179342   20626 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1764616260.190968   20626 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Evaluating TFLite Model: 100%|██████████| 10000/10000 [00:00<00:00, 24297.69it/s]\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- STAGE 2: Running Float32 (No Quantization) Comparison ---\")\n",
    "\n",
    "# TFLite (Float)\n",
    "tflite_float_model = convert_tf_to_tflite(tf_model, QuantizationScheme.NONE)\n",
    "tflite_float_acc = get_tflite_model_accuracy(tflite_float_model, tf_test_data, QuantizationScheme.NONE)\n",
    "tflite_float_size = len(tflite_float_model)\n",
    "results.append((\"TFLite (Float32)\", tflite_float_acc, tflite_float_size))\n",
    "\n",
    "# DMC (Float)\n",
    "dmc_float_model = dmc_base_model.init_compress({\n",
    "    \"quantize\": {\"scheme\": QuantizationScheme.NONE, \"bitwidth\": None, \"granularity\": None}\n",
    "    }, INPUT_SHAPE_TORCH)\n",
    "dmc_float_eval = dmc_float_model.evaluate(torch_test_loader, dmc_metrics, device=DEVICE)\n",
    "dmc_float_size = dmc_float_model.get_size_in_bits() // 8\n",
    "results.append((\"DMC (Float32)\", dmc_float_eval['acc'], dmc_float_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce927f7",
   "metadata": {},
   "source": [
    "### DYNAMIC QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda4887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 3: Running Dynamic Quantization Comparison ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp_ov_vri1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_ov_vri1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp_ov_vri1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127492804330896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804333584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804332432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804333968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804331664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804333392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804335504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804336080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804336464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804337424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804337232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804336656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804335312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804338000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804338768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804337616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1764616262.283250   20626 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1764616262.283270   20626 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "Evaluating TFLite Model: 100%|██████████| 10000/10000 [00:00<00:00, 31265.66it/s]\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- STAGE 3: Running Dynamic Quantization Comparison ---\")\n",
    "\n",
    "# TFLite (Dynamic)\n",
    "tflite_dyn_model = convert_tf_to_tflite(tf_model, QuantizationScheme.DYNAMIC)\n",
    "tflite_dyn_acc = get_tflite_model_accuracy(tflite_dyn_model, tf_test_data, QuantizationScheme.DYNAMIC)\n",
    "tflite_dyn_size = len(tflite_dyn_model)\n",
    "results.append((\"TFLite (Dynamic)\", tflite_dyn_acc, tflite_dyn_size))\n",
    "\n",
    "# DMC (Dynamic)\n",
    "dmc_dyn_model = dmc_base_model.init_compress({\n",
    "    \"quantize\": {\"scheme\": QuantizationScheme.DYNAMIC, \"bitwidth\": 8, \"granularity\": QuantizationGranularity.PER_TENSOR}\n",
    "}, INPUT_SHAPE_TORCH)\n",
    "dmc_dyn_eval = dmc_dyn_model.evaluate(torch_test_loader, dmc_metrics, device=DEVICE)\n",
    "dmc_dyn_size = dmc_dyn_model.get_size_in_bits() // 8\n",
    "results.append((\"DMC (Dynamic)\", dmc_dyn_eval['acc'], dmc_dyn_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b030b",
   "metadata": {},
   "source": [
    "### STAGE 3: STATIC QUANTIZATION \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed6ce6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 4: Running Static Quantization (INT8) Comparison ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp9bi0uzln/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9bi0uzln/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp9bi0uzln'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127492804330896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804333584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804332432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804333968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804331664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804333392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804335504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804336080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804336464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804337424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804337232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804336656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804335312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804338000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804338768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127492804337616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1764616264.475632   20626 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1764616264.475657   20626 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n",
      "Evaluating TFLite Model: 100%|██████████| 10000/10000 [00:00<00:00, 20754.92it/s]\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- STAGE 4: Running Static Quantization (INT8) Comparison ---\")\n",
    "\n",
    "# TFLite (Static)\n",
    "tflite_static_model = convert_tf_to_tflite(tf_model, QuantizationScheme.STATIC, tf_train_data)\n",
    "tflite_static_acc = get_tflite_model_accuracy(tflite_static_model, tf_test_data, QuantizationScheme.STATIC)\n",
    "tflite_static_size = len(tflite_static_model)\n",
    "results.append((\"TFLite (Static)\", tflite_static_acc, tflite_static_size))\n",
    "\n",
    "# DMC (Static)\n",
    "calib_data_torch = next(iter(torch_train_loader))[0].to(DEVICE)\n",
    "dmc_static_model = dmc_base_model.init_compress({\n",
    "    \"quantize\": {\"scheme\": QuantizationScheme.STATIC, \"bitwidth\": 8, \"granularity\": QuantizationGranularity.PER_TENSOR}\n",
    "}, INPUT_SHAPE_TORCH, calibration_data=calib_data_torch)\n",
    "dmc_static_eval = dmc_static_model.evaluate(torch_test_loader, dmc_metrics, device=DEVICE)\n",
    "dmc_static_size = dmc_static_model.get_size_in_bits() // 8\n",
    "results.append((\"DMC (Static)\", dmc_static_eval['acc'], dmc_static_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00798ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- REPRODUCTION FINISHED: TFLITE vs. DMC (TABLE24) ---\n",
      "============================================================\n",
      "Method               | Accuracy (%)    | Size (KB)      \n",
      "------------------------------------------------------------\n",
      "TFLite (Float32)     | 98.91           | 148.43         \n",
      "DMC (Float32)        | 3162.56         | 145.12         \n",
      "TFLite (Dynamic)     | 98.91           | 43.98          \n",
      "DMC (Dynamic)        | 3162.56         | 36.76          \n",
      "TFLite (Static)      | 98.88           | 43.09          \n",
      "DMC (Static)         | 3160.96         | 36.79          \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Print Final Summary Table ---\n",
    "print(\"\\n\\n--- REPRODUCTION FINISHED: TFLITE vs. DMC (TABLE24) ---\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':<20} | {'Accuracy (%)':<15} | {'Size (KB)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for name, acc, size in results:\n",
    "    print(f\"{name:<20} | {acc * 100:<15.2f} | {size/1024:<15.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
