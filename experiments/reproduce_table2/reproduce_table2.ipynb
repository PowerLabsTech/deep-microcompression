{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4388a3",
   "metadata": {},
   "source": [
    "# TFLite vs. DMC Comparison Reproduction Guide\n",
    "\n",
    "This guide explains how to reproduce the TFLite vs. DMC comparison (Table 2) from the \"Deep Microcompression\" paper.\n",
    "\n",
    "## Required File Structure\n",
    "\n",
    "This script assumes it is located within the original project's directory structure under the experiments directory. The development module must be accessible two levels up.\n",
    "\n",
    "\n",
    "## What to Expect\n",
    "\n",
    "The script will run the full comparison, which involves:\n",
    "\n",
    "1. Baseline TF Model: Trains the TensorFlow/Keras LeNet-5 model (25 epochs) and saves it as lenet5_model.keras. This is the single source for both TFLite and DMC models.\n",
    "\n",
    "1. Comparison Stage 1 (Float32): Compares the accuracy and size of the TFLite Float32 model vs. the DMC Float32 model.\n",
    "\n",
    "1. Comparison Stage 2 (Dynamic Quantization): Compares TFLite Dynamic Quantization vs. DMC Dynamic Quantization.\n",
    "\n",
    "1. Comparison Stage 3 (Static Quantization): Compares TFLite Static INT8 Quantization vs. DMC Static INT8 Quantization.\n",
    "\n",
    "The script will print the Accuracy and Model Size for all six models, finishing with a summary table that directly reproduces the data for Table 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b490d8",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56f4f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Force TensorFlow to CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb088eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    import torch\n",
    "    from torch.utils import data\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.datasets import mnist # type: ignore\n",
    "\n",
    "except:\n",
    "    %pip install torch torchvision tensorflow tqdm\n",
    "    import numpy as np\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    import torch\n",
    "    from torch.utils import data\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.datasets import mnist # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "271c082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes the script is in 'project_root/experiments/reproduce_table'\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "try:\n",
    "    from development import (\n",
    "        Sequential, Conv2d, Linear, ReLU, BatchNorm2d,\n",
    "        MaxPool2d, Flatten, QuantizationScheme, QuantizationGranularity\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"Error: Could not import the 'development' module.\")\n",
    "    print(\"Please ensure this script is run from 'experiments/lenet5/'\")\n",
    "    print(\"and the 'development' module is in the project root ('../../../').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebf0c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "DEVICE = \"cpu\"  # Force PyTorch to CPU for fair comparison\n",
    "\n",
    "LUCKY_NUMBER = 25\n",
    "LENET5_TF_FILE = \"lenet5_model.keras\"\n",
    "INPUT_SHAPE_TORCH = (1, 28, 28)\n",
    "INPUT_SHAPE_TF = (28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "454779e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(LUCKY_NUMBER)\n",
    "tf.random.set_seed(LUCKY_NUMBER)\n",
    "np.random.seed(LUCKY_NUMBER)\n",
    "random.seed(LUCKY_NUMBER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ddc6a",
   "metadata": {},
   "source": [
    "### Getting the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfb8fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "def get_data_loaders():\n",
    "    \"\"\"Loads MNIST data for both TF/Numpy and PyTorch.\"\"\"\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    (mnist_train_image, mnist_train_label), (mnist_test_image, mnist_test_label) = mnist.load_data()\n",
    "    \n",
    "    # TF/Numpy data (normalized, reshaped)\n",
    "    np_train_img = (mnist_train_image / 255.0).astype(np.float32).reshape(-1, 28, 28, 1)\n",
    "    np_train_lbl = mnist_train_label.astype(np.int64)\n",
    "    np_test_img = (mnist_test_image / 255.0).astype(np.float32).reshape(-1, 28, 28, 1)\n",
    "    np_test_lbl = mnist_test_label.astype(np.int64)\n",
    "\n",
    "    # PyTorch data (needs channel-first)\n",
    "    torch_train_img = torch.from_numpy(np_train_img.transpose(0, 3, 1, 2))\n",
    "    torch_train_lbl = torch.from_numpy(np_train_lbl)\n",
    "    torch_test_img = torch.from_numpy(np_test_img.transpose(0, 3, 1, 2))\n",
    "    torch_test_lbl = torch.from_numpy(np_test_lbl)\n",
    "\n",
    "    torch_train_dataset = data.TensorDataset(torch_train_img, torch_train_lbl)\n",
    "    torch_test_dataset = data.TensorDataset(torch_test_img, torch_test_lbl)\n",
    "    \n",
    "    torch_train_loader = data.DataLoader(torch_train_dataset, batch_size=32, shuffle=True)\n",
    "    torch_test_loader = data.DataLoader(torch_test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return (np_train_img, np_train_lbl), (np_test_img, np_test_lbl), torch_train_loader, torch_test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249f31b",
   "metadata": {},
   "source": [
    "### Defining and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30952277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. TF Model ---\n",
    "def get_tf_model(train_data, test_data):\n",
    "    \"\"\"Trains or loads the baseline TF/Keras model.\"\"\"\n",
    "    print(\"\\n--- STAGE 1: Training/Loading TF Baseline Model ---\")\n",
    "    (train_img, train_lbl) = train_data\n",
    "    (test_img, test_lbl) = test_data\n",
    "    \n",
    "    if os.path.exists(LENET5_TF_FILE):\n",
    "        print(f\"Loading existing TF model from {LENET5_TF_FILE}...\")\n",
    "        return tf.keras.models.load_model(LENET5_TF_FILE)\n",
    "    \n",
    "    print(\"No TF model found. Training from scratch (up to 25 epochs)...\")\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=INPUT_SHAPE_TF),\n",
    "        tf.keras.layers.ZeroPadding2D(padding=2),\n",
    "        tf.keras.layers.Conv2D(filters=6, kernel_size=5, padding=\"valid\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=5, padding=\"valid\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=84),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Dense(units=10)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        train_img, train_lbl,\n",
    "        epochs=25,\n",
    "        batch_size=32, \n",
    "        validation_data=(test_img, test_lbl),\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving TF model to {LENET5_TF_FILE}...\")\n",
    "    model.save(LENET5_TF_FILE)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50312b45",
   "metadata": {},
   "source": [
    "### Helper function for comparing TFLite and DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a74f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TF to DMC (PyTorch) Converter ---\n",
    "@torch.no_grad()\n",
    "def copy_tensor(tensor_source, tensor_destination):\n",
    "    tensor_destination.copy_(tensor_source)\n",
    "\n",
    "@torch.no_grad()\n",
    "def convert_tf_to_dmc(tf_model):\n",
    "    \"\"\"\n",
    "    Converts the trained Keras model to the DMC Sequential model.\n",
    "    It implements layer conversion for just layers needed for vgg13.\n",
    "    \"\"\"\n",
    "    print(\"Converting TF model to DMC (PyTorch) model...\")\n",
    "    prev_layer_is_flatten = False\n",
    "    last_conv_channel = None\n",
    "    dmc_layers = []\n",
    "    pad_next_conv = None\n",
    "\n",
    "    for layer in tf_model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            pass\n",
    "        elif isinstance(layer, tf.keras.layers.ZeroPadding2D):\n",
    "            pad_next_conv = layer.padding\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            last_conv_channel = layer.weights[0].shape[-1]\n",
    "            weight_np = np.transpose(layer.weights[0].numpy(), (3, 2, 0, 1)) # TF(kH,kW,in,out) -> Torch(out,in,kH,kW)\n",
    "            out_channels, in_channels, kernel_size, _ = weight_np.shape\n",
    "            stride = layer.strides[0]\n",
    "            padding_str = layer.padding\n",
    "            pad = [0]*4 if padding_str == \"valid\" else [(kernel_size - 1)//2]*4\n",
    "            if pad_next_conv is not None:\n",
    "                for i, padding in enumerate(pad_next_conv):\n",
    "                    pad[i*2] = padding[0]\n",
    "                    pad[i*2 + 1] = padding[1]\n",
    "                pad_next_conv = None\n",
    "            has_bias = len(layer.weights) > 1\n",
    "            conv_layer = Conv2d(in_channels, out_channels, kernel_size, stride, pad=pad, bias=has_bias)\n",
    "            if has_bias:\n",
    "                copy_tensor(torch.from_numpy(layer.weights[1].numpy()), conv_layer.bias)\n",
    "            copy_tensor(torch.from_numpy(weight_np), conv_layer.weight)\n",
    "            dmc_layers.append(conv_layer)\n",
    "\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            weight_np = layer.weights[0].numpy() # TF(in, out)\n",
    "            if prev_layer_is_flatten:\n",
    "                # Handle TF's channel-last flatten for Dense layer\n",
    "                weight_np = weight_np.reshape(-1, last_conv_channel, weight_np.shape[-1])\n",
    "                weight_np = np.transpose(weight_np, (1, 0, 2))\n",
    "                weight_np = weight_np.reshape(-1, weight_np.shape[-1])\n",
    "                prev_layer_is_flatten = False\n",
    "            \n",
    "            weight_np = np.transpose(weight_np, (1, 0)) # TF(in,out) -> Torch(out,in)\n",
    "            out_features, in_features = weight_np.shape\n",
    "            \n",
    "            linear_layer = Linear(out_features=out_features, in_features=in_features, bias=True)\n",
    "            copy_tensor(torch.from_numpy(layer.weights[1].numpy()), linear_layer.bias)\n",
    "            copy_tensor(torch.from_numpy(weight_np), linear_layer.weight)\n",
    "            dmc_layers.append(linear_layer)         \n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            bn_layer = BatchNorm2d(num_features=layer.gamma.shape[0], affine=layer.scale or layer.center, eps=layer.epsilon, momentum=(1 - layer.momentum))\n",
    "            copy_tensor(torch.from_numpy(layer.gamma.numpy()), bn_layer.weight)\n",
    "            copy_tensor(torch.from_numpy(layer.beta.numpy()), bn_layer.bias)\n",
    "            copy_tensor(torch.from_numpy(layer.moving_mean.numpy()), bn_layer.running_mean)\n",
    "            copy_tensor(torch.from_numpy(layer.moving_variance.numpy()), bn_layer.running_var)\n",
    "            dmc_layers.append(bn_layer)\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.ReLU):\n",
    "            dmc_layers.append(ReLU())\n",
    "        elif isinstance(layer, tf.keras.layers.Flatten):\n",
    "            prev_layer_is_flatten = True\n",
    "            dmc_layers.append(Flatten())\n",
    "        elif isinstance(layer, tf.keras.layers.MaxPool2D):\n",
    "            stride = layer.strides[0]\n",
    "            kernel_size = layer.pool_size[0]\n",
    "            dmc_layers.append(MaxPool2d(kernel_size=kernel_size, stride=stride))\n",
    "        else: \n",
    "            raise RuntimeError(f\"Unknown layer type: {type(layer)}\")\n",
    "        \n",
    "    return Sequential(*dmc_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2552beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. TFLite Helper Functions ---\n",
    "def convert_tf_to_tflite(tf_model, scheme=QuantizationScheme.NONE, test_data=None):\n",
    "    \"\"\"Converts Keras model to TFLite flatbuffer.\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\n",
    "    \n",
    "    if scheme == QuantizationScheme.DYNAMIC:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    if scheme == QuantizationScheme.STATIC:        \n",
    "        (train_img, _) = test_data\n",
    "        def representative_dataset():\n",
    "            for i in range(100): # Use 100 batches for calibration\n",
    "                yield [train_img[i*32:(i+1)*32]]\n",
    "                \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "\n",
    "    return converter.convert()\n",
    "\n",
    "def get_tflite_model_accuracy(tflite_model, test_data, scheme=QuantizationScheme.NONE):\n",
    "    \"\"\"Evaluates a TFLite flatbuffer model.\"\"\"\n",
    "    (_, test_lbl) = test_data\n",
    "    (test_img, _) = test_data\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    tflite_predicted = []\n",
    "    \n",
    "    for image in tqdm(test_img, desc=\"Evaluating TFLite Model\"):\n",
    "        if scheme == QuantizationScheme.STATIC:\n",
    "            scale, zero_point = input_details[\"quantization\"]\n",
    "            image = ((image / scale) + zero_point).astype(np.int8)\n",
    "        \n",
    "        interpreter.set_tensor(input_details[\"index\"], image.reshape(1, 28, 28, 1))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[\"index\"])\n",
    "        tflite_predicted.append(np.argmax(output_data))\n",
    "\n",
    "    tflite_predicted = np.array(tflite_predicted)\n",
    "    return (tflite_predicted == test_lbl).sum() / len(test_lbl)\n",
    "\n",
    "# --- 5. PyTorch (DMC) Accuracy Helper ---\n",
    "def accuracy_fun(y_pred, y_true):\n",
    "    return (y_pred.argmax(dim=1) == y_true).to(torch.float).mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1744c197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "\n",
      "--- STAGE 1: Training/Loading TF Baseline Model ---\n",
      "Loading existing TF model from lenet5_model.keras...\n",
      "Converting TF model to DMC (PyTorch) model...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# --- Load Data ---\n",
    "tf_train_data, tf_test_data, torch_train_loader, torch_test_loader = get_data_loaders()\n",
    "\n",
    "# --- Get Baseline Models ---\n",
    "tf_model = get_tf_model(tf_train_data, tf_test_data)\n",
    "dmc_base_model = convert_tf_to_dmc(tf_model).to(DEVICE)\n",
    "dmc_metrics = {\"acc\": accuracy_fun}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37cb35a",
   "metadata": {},
   "source": [
    "### NO QUANTIZATION (Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3821e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2: Running Float32 (No Quantization) Comparison ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpfg2059w5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpfg2059w5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpfg2059w5'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140254374715344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374716304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374723216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374707664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374710544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374716880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374716688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374719376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374708624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374712272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374713616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1764763725.806956   25704 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1764763725.806990   25704 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "Evaluating TFLite Model: 100%|██████████| 10000/10000 [00:00<00:00, 17976.62it/s]\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- STAGE 2: Running Float32 (No Quantization) Comparison ---\")\n",
    "\n",
    "# TFLite (Float)\n",
    "tflite_float_model = convert_tf_to_tflite(tf_model, QuantizationScheme.NONE)\n",
    "tflite_float_acc = get_tflite_model_accuracy(tflite_float_model, tf_test_data, QuantizationScheme.NONE)\n",
    "tflite_float_size = len(tflite_float_model)\n",
    "results.append((\"TFLite (Float32)\", tflite_float_acc, tflite_float_size))\n",
    "\n",
    "# DMC (Float)\n",
    "dmc_float_model = dmc_base_model.init_compress({\n",
    "    \"quantize\": {\"scheme\": QuantizationScheme.NONE, \"bitwidth\": None, \"granularity\": None}\n",
    "    }, INPUT_SHAPE_TORCH)\n",
    "dmc_float_eval = dmc_float_model.evaluate(torch_test_loader, dmc_metrics, device=DEVICE)\n",
    "dmc_float_size = dmc_float_model.get_size_in_bits() // 8\n",
    "results.append((\"DMC (Float32)\", dmc_float_eval['acc'], dmc_float_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce927f7",
   "metadata": {},
   "source": [
    "### DYNAMIC QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eda4887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 3: Running Dynamic Quantization Comparison ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpuq7wm9hw/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuq7wm9hw/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpuq7wm9hw'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140254374715344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374716304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374723216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374707664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374710544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374716880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374716688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374719376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374708624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374712272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374713616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1764763728.530473   25704 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1764763728.530499   25704 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "Evaluating TFLite Model: 100%|██████████| 10000/10000 [00:00<00:00, 28935.51it/s]\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- STAGE 3: Running Dynamic Quantization Comparison ---\")\n",
    "\n",
    "# TFLite (Dynamic)\n",
    "tflite_dyn_model = convert_tf_to_tflite(tf_model, QuantizationScheme.DYNAMIC)\n",
    "tflite_dyn_acc = get_tflite_model_accuracy(tflite_dyn_model, tf_test_data, QuantizationScheme.DYNAMIC)\n",
    "tflite_dyn_size = len(tflite_dyn_model)\n",
    "results.append((\"TFLite (Dynamic)\", tflite_dyn_acc, tflite_dyn_size))\n",
    "\n",
    "# DMC (Dynamic)\n",
    "dmc_dyn_model = dmc_base_model.init_compress({\n",
    "    \"quantize\": {\"scheme\": QuantizationScheme.DYNAMIC, \"bitwidth\": 8, \"granularity\": QuantizationGranularity.PER_TENSOR}\n",
    "}, INPUT_SHAPE_TORCH)\n",
    "dmc_dyn_eval = dmc_dyn_model.evaluate(torch_test_loader, dmc_metrics, device=DEVICE)\n",
    "dmc_dyn_size = dmc_dyn_model.get_size_in_bits() // 8\n",
    "results.append((\"DMC (Dynamic)\", dmc_dyn_eval['acc'], dmc_dyn_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b030b",
   "metadata": {},
   "source": [
    "### STAGE 3: STATIC QUANTIZATION \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed6ce6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 4: Running Static Quantization (INT8) Comparison ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpb14wfro1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpb14wfro1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpb14wfro1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140254374715344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374716304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374723216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374707664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374710544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374716880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374716688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374719376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374708624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374709968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374712272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140254374713616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1764763731.061112   25704 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1764763731.061128   25704 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n",
      "Evaluating TFLite Model: 100%|██████████| 10000/10000 [00:00<00:00, 20760.14it/s]\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- STAGE 4: Running Static Quantization (INT8) Comparison ---\")\n",
    "\n",
    "# TFLite (Static)\n",
    "tflite_static_model = convert_tf_to_tflite(tf_model, QuantizationScheme.STATIC, tf_train_data)\n",
    "tflite_static_acc = get_tflite_model_accuracy(tflite_static_model, tf_test_data, QuantizationScheme.STATIC)\n",
    "tflite_static_size = len(tflite_static_model)\n",
    "results.append((\"TFLite (Static)\", tflite_static_acc, tflite_static_size))\n",
    "\n",
    "# DMC (Static)\n",
    "calib_data_torch = next(iter(torch_train_loader))[0].to(DEVICE)\n",
    "dmc_static_model = dmc_base_model.init_compress({\n",
    "    \"quantize\": {\"scheme\": QuantizationScheme.STATIC, \"bitwidth\": 8, \"granularity\": QuantizationGranularity.PER_TENSOR}\n",
    "}, INPUT_SHAPE_TORCH, calibration_data=calib_data_torch)\n",
    "dmc_static_eval = dmc_static_model.evaluate(torch_test_loader, dmc_metrics, device=DEVICE)\n",
    "dmc_static_size = dmc_static_model.get_size_in_bits() // 8\n",
    "results.append((\"DMC (Static)\", dmc_static_eval['acc'], dmc_static_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2628d",
   "metadata": {},
   "source": [
    "### Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00798ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- REPRODUCTION FINISHED: TFLITE vs. DMC (TABLE24) ---\n",
      "============================================================\n",
      "       Method        |  Accuracy (%)   |    Size (KB)   \n",
      "------------------------------------------------------------\n",
      "  TFLite (Float32)   |      98.91      |     148.43     \n",
      "   DMC (Float32)     |      98.91      |     145.12     \n",
      "  TFLite (Dynamic)   |      98.91      |      43.98     \n",
      "   DMC (Dynamic)     |      98.91      |      36.76     \n",
      "  TFLite (Static)    |      98.88      |      43.09     \n",
      "    DMC (Static)     |      98.86      |      36.79     \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Print Final Summary Table ---\n",
    "print(\"\\n\\n--- REPRODUCTION FINISHED: TFLITE vs. DMC (TABLE24) ---\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':^20} | {'Accuracy (%)':^15} | {'Size (KB)':^15}\")\n",
    "print(\"-\" * 60)\n",
    "for name, acc, size in results:\n",
    "    print(f\"{name:^20} | {acc * 100:^15.2f} | {size/1024:^15.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae3997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
