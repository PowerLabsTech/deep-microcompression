{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2730641c",
   "metadata": {},
   "source": [
    "# LeNet-5 Experiment Reproduction Guide\n",
    "\n",
    "This guide explains how to reproduce the LeNet-5 (Baseline, Pruned, and Quantized-Pruned) experiments from the \"Deep Microcompression\" paper.\n",
    "\n",
    "## Required File Structure\n",
    "\n",
    "This script assumes it is located within the original project's directory structure under the experiments directory. The development module must be accessible two levels up from this script.\n",
    "\n",
    "\n",
    "## What to Expect\n",
    "\n",
    "The script will run the full experiment, which involves three stages:\n",
    "\n",
    "1. Baseline Model: Trains the original LeNet-5 model (20 epochs with early stopping) and saves it as lenet5_state_dict.pth.\n",
    "\n",
    "1. Pruned Model: Loads the baseline weights, applies the optimal structured pruning (conv2d_1: 9, linear_0: 50), and retrains the model (20 epochs).\n",
    "\n",
    "1. Quantized-Pruned Model: Applies 4-bit static quantization to the pruned model and performs Quantization-Aware Training (QAT) (15 epochs).\n",
    "\n",
    "The script will print the Accuracy and Model Size for each of these three stages, allowing your supervisor to easily verify the results from Table 2 in your paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b666c2",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12010403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch import nn, optim\n",
    "    from torch.utils import data\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "except ImportError:\n",
    "    %pip install torch torchvision tqdm\n",
    "    import torch\n",
    "    from torch import nn, optim\n",
    "    from torch.utils import data\n",
    "    from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5e75a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/Documents/EmbeddedAI/deep-microcompression/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# This assumes the script is in 'project_root/experiments/reproduce_table1'\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "try:\n",
    "    from development import (\n",
    "        Sequential,\n",
    "        Conv2d,\n",
    "        BatchNorm2d,\n",
    "        ReLU,\n",
    "        MaxPool2d,\n",
    "        Flatten,\n",
    "        Linear,\n",
    "        EarlyStopper,\n",
    "        QuantizationGranularity,\n",
    "        QuantizationScheme\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"Error: Could not import the 'development' module.\")\n",
    "    print(\"Please ensure this script is run from 'experiments/reproduce_table1/'\")\n",
    "    print(\"and the 'development' module is in the project root ('../../').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335eb974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for a GPU, defaults to the cpu\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# To load a trained model, to skip the initial training step\n",
    "BASELINE_MODEL_FILE = \"lenet5_state_dict.pth\"\n",
    "INPUT_SHAPE = (1, 28, 28)\n",
    "DATASET_DIR = \"../../Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434b340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LUCKY_NUMBER = 25\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(LUCKY_NUMBER)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed(LUCKY_NUMBER)\n",
    "# cuDNN determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3837a",
   "metadata": {},
   "source": [
    "### Getting the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e43df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "def get_data_loaders():\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.RandomCrop((24, 24)),\n",
    "        transforms.Resize(INPUT_SHAPE[1:]),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    mnist_train_dataset = datasets.MNIST(DATASET_DIR, train=True, download=True, transform=data_transform)\n",
    "    mnist_test_dataset = datasets.MNIST(DATASET_DIR, train=False, download=True, transform=data_transform)\n",
    "    \n",
    "    mnist_train_loader = data.DataLoader(mnist_train_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count(), drop_last=False) # type: ignore\n",
    "    mnist_test_loader = data.DataLoader(mnist_test_dataset, batch_size=32, shuffle=False, num_workers=os.cpu_count(), drop_last=False) # type: ignore\n",
    "    \n",
    "    return mnist_train_loader, mnist_test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052a03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Training & Evaluation Functions ---\n",
    "def accuracy_fun(y_pred, y_true):\n",
    "    return (y_pred.argmax(dim=1) == y_true).to(torch.float).mean().item() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b09d4",
   "metadata": {},
   "source": [
    "### Defining and Training the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb803c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define Model ---\n",
    "def get_baseline_model():\n",
    "    return Sequential(\n",
    "        Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, pad=(2, 2, 2, 2), bias=True),\n",
    "        BatchNorm2d(num_features=6),\n",
    "        ReLU(),\n",
    "        MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True),\n",
    "        BatchNorm2d(num_features=16),\n",
    "        ReLU(),\n",
    "        MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        Flatten(),\n",
    "        Linear(in_features=16*5*5, out_features=84, bias=True),\n",
    "        ReLU(),\n",
    "        Linear(in_features=84, out_features=10, bias=True)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "\n",
    "def train_baseline(model, train_loader, test_loader):\n",
    "    print(\"\\n--- STAGE 1: Training Baseline Model ---\")\n",
    "    if os.path.exists(BASELINE_MODEL_FILE):\n",
    "        print(f\"Loading existing baseline weights from {BASELINE_MODEL_FILE}...\")\n",
    "        model.load_state_dict(torch.load(BASELINE_MODEL_FILE, weights_only=True), strict=False)\n",
    "        model.to(DEVICE)\n",
    "        return model\n",
    "\n",
    "    print(f\"No baseline weights found. Training from scratch (up to 15 epochs)...\")\n",
    "    early_stopper = EarlyStopper(\n",
    "        monitor_metric=\"validation_loss\",\n",
    "        delta=1e-7,\n",
    "        mode=\"min\",\n",
    "        patience=4,\n",
    "        restore_best_state_dict=True,\n",
    "    )\n",
    "    criterion_fun = nn.CrossEntropyLoss()\n",
    "    optimizer_fun = optim.Adam(model.parameters(), lr=1.e-3)\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_fun, mode=\"min\", patience=2)\n",
    "\n",
    "    model.fit(\n",
    "        train_loader, 15, \n",
    "        criterion_fun, optimizer_fun, lr_scheduler,\n",
    "        validation_dataloader=test_loader, \n",
    "        metrics={\"acc\": accuracy_fun},\n",
    "        callbacks=[early_stopper],\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving baseline weights to {BASELINE_MODEL_FILE}...\")\n",
    "    torch.save(model.cpu().state_dict(), BASELINE_MODEL_FILE)\n",
    "    model.to(DEVICE)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb974d",
   "metadata": {},
   "source": [
    "### Pruning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c61fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pruned(baseline_model, train_loader, test_loader):\n",
    "    print(\"\\n--- STAGE 2: Applying Pruning & Retraining ---\")\n",
    "    \n",
    "    # Pruning parameters from paper (Table 1 / Sec 4.1.1)\n",
    "    pruning_config = {\n",
    "        \"prune_channel\": {\n",
    "            \"sparsity\": {\n",
    "                \"conv2d_0\": 0,\n",
    "                \"conv2d_1\": 9,\n",
    "                \"linear_0\": 64\n",
    "            },\n",
    "            \"metric\": \"l2\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Applying pruning config: {pruning_config['prune_channel']['sparsity']}\")\n",
    "    \n",
    "    # Re-initialize model architecture with pruning\n",
    "    pruned_model = baseline_model.init_compress(pruning_config, INPUT_SHAPE).to(DEVICE)\n",
    "    \n",
    "    # Retrain (fine-tune) the pruned model\n",
    "    print(\"Retraining pruned model (15 epochs)...\")\n",
    "    criterion_fun = nn.CrossEntropyLoss()\n",
    "    optimizer_fun = optim.SGD(pruned_model.parameters(), lr=1.e-3, weight_decay=5e-4, momentum=.9)\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_fun, mode=\"min\", patience=1)\n",
    "\n",
    "    pruned_model.fit(\n",
    "        train_loader, 20, \n",
    "        criterion_fun, optimizer_fun, lr_scheduler,\n",
    "        validation_dataloader=test_loader, \n",
    "        metrics={\"acc\": accuracy_fun},\n",
    "        device=DEVICE\n",
    "    )\n",
    "    return pruned_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badce33f",
   "metadata": {},
   "source": [
    "### Quantizing and Bit Packing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5606b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_quantized_pruned(pruned_model, train_loader, test_loader):\n",
    "    print(\"\\n--- STAGE 3: Applying Quantization (QAT) & Retraining ---\")\n",
    "    \n",
    "    # Configs from paper (Table 2, 4-bit static) and notebook\n",
    "    pruning_config = {\n",
    "        \"prune_channel\": {\n",
    "            \"sparsity\": {\n",
    "                \"conv2d_0\": 0,\n",
    "                \"conv2d_1\": 9,\n",
    "                \"linear_0\": 64\n",
    "            },\n",
    "            \"metric\": \"l2\"\n",
    "        }\n",
    "    }\n",
    "    quantization_config = {\n",
    "        \"quantize\": {\n",
    "            \"scheme\": QuantizationScheme.STATIC,\n",
    "            \"granularity\": QuantizationGranularity.PER_TENSOR,\n",
    "            \"bitwidth\": 4\n",
    "        }\n",
    "    }\n",
    "    full_compression_config = {**pruning_config, **quantization_config}\n",
    "\n",
    "    print(f\"Applying quantization config: 4-bit, STATIC, PER_TENSOR\")\n",
    "    \n",
    "    # Get one batch of calibration data\n",
    "    calibration_data = next(iter(test_loader))[0].to(DEVICE)\n",
    "    \n",
    "    # Initialize compression for QAT\n",
    "    quantized_model = pruned_model.init_compress(\n",
    "        full_compression_config, \n",
    "        INPUT_SHAPE, \n",
    "        calibration_data\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Perform Quantization-Aware Training (15 epochs from paper Table 2)\n",
    "    print(\"Performing QAT (15 epochs)...\")\n",
    "    criterion_fun = nn.CrossEntropyLoss()\n",
    "    optimizer_fun = optim.SGD(quantized_model.parameters(), lr=1.e-4, weight_decay=5e-4, momentum=.9)\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_fun, mode=\"min\", patience=1)\n",
    "\n",
    "    quantized_model.fit(\n",
    "        train_loader, 15, \n",
    "        criterion_fun, optimizer_fun, lr_scheduler,\n",
    "        validation_dataloader=test_loader, \n",
    "        metrics={\"acc\": accuracy_fun},\n",
    "        device=DEVICE\n",
    "    )\n",
    "    return quantized_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1202d",
   "metadata": {},
   "source": [
    "### Baseline Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b831b07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "\n",
      "--- STAGE 1: Training Baseline Model ---\n",
      "Loading existing baseline weights from lenet5_state_dict.pth...\n",
      "Evaluating baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> STAGE 1 (Baseline) COMPLETE ==\n",
      "    Accuracy: 99.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "train_loader, test_loader = get_data_loaders()\n",
    "\n",
    "# --- STAGE 1: BASELINE ---\n",
    "baseline_model = get_baseline_model()\n",
    "baseline_model = train_baseline(baseline_model, train_loader, test_loader)\n",
    "\n",
    "print(\"Evaluating baseline model...\")\n",
    "baseline_eval = baseline_model.evaluate(test_loader, {\"acc\": accuracy_fun}, device=DEVICE)\n",
    "print(f\"==> STAGE 1 (Baseline) COMPLETE ==\")\n",
    "print(f\"    Accuracy: {baseline_eval['acc']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e9cef",
   "metadata": {},
   "source": [
    "### Pruned Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf2c0cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2: Applying Pruning & Retraining ---\n",
      "Applying pruning config: {'conv2d_0': 0, 'conv2d_1': 9, 'linear_0': 64}\n",
      "Retraining pruned model (15 epochs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):   5%|▌         | 1/20 [00:15<05:02, 15.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0 | train loss 0.2733 | validation loss 0.1073 | train acc 92.0550 | validation acc 96.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  10%|█         | 2/20 [00:31<04:44, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 | train loss 0.1048 | validation loss 0.0873 | train acc 96.8150 | validation acc 97.4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  15%|█▌        | 3/20 [00:49<04:46, 16.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    2 | train loss 0.0873 | validation loss 0.0689 | train acc 97.3367 | validation acc 97.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  20%|██        | 4/20 [01:07<04:32, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    3 | train loss 0.0792 | validation loss 0.0678 | train acc 97.6200 | validation acc 97.9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  25%|██▌       | 5/20 [01:24<04:16, 17.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    4 | train loss 0.0721 | validation loss 0.0583 | train acc 97.7833 | validation acc 98.2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  30%|███       | 6/20 [01:41<03:58, 17.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    5 | train loss 0.0677 | validation loss 0.0600 | train acc 97.7983 | validation acc 98.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  35%|███▌      | 7/20 [01:58<03:42, 17.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    6 | train loss 0.0647 | validation loss 0.0531 | train acc 97.9600 | validation acc 98.3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  40%|████      | 8/20 [02:15<03:24, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    7 | train loss 0.0610 | validation loss 0.0578 | train acc 98.1300 | validation acc 98.0900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  45%|████▌     | 9/20 [02:32<03:09, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    8 | train loss 0.0595 | validation loss 0.0477 | train acc 98.1183 | validation acc 98.5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  50%|█████     | 10/20 [02:49<02:51, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    9 | train loss 0.0599 | validation loss 0.0572 | train acc 98.1167 | validation acc 98.3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  55%|█████▌    | 11/20 [03:06<02:34, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   10 | train loss 0.0572 | validation loss 0.0492 | train acc 98.2300 | validation acc 98.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  60%|██████    | 12/20 [03:23<02:15, 16.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   11 | train loss 0.0531 | validation loss 0.0497 | train acc 98.3617 | validation acc 98.3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  65%|██████▌   | 13/20 [03:40<01:58, 16.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   12 | train loss 0.0531 | validation loss 0.0476 | train acc 98.3900 | validation acc 98.3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  70%|███████   | 14/20 [03:56<01:40, 16.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   13 | train loss 0.0532 | validation loss 0.0461 | train acc 98.3117 | validation acc 98.4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  75%|███████▌  | 15/20 [04:13<01:23, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   14 | train loss 0.0518 | validation loss 0.0484 | train acc 98.3933 | validation acc 98.3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  80%|████████  | 16/20 [04:29<01:06, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   15 | train loss 0.0518 | validation loss 0.0478 | train acc 98.3900 | validation acc 98.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  85%|████████▌ | 17/20 [04:46<00:49, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   16 | train loss 0.0510 | validation loss 0.0446 | train acc 98.4300 | validation acc 98.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  90%|█████████ | 18/20 [05:03<00:33, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   17 | train loss 0.0516 | validation loss 0.0502 | train acc 98.4150 | validation acc 98.3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20):  95%|█████████▌| 19/20 [05:19<00:16, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   18 | train loss 0.0515 | validation loss 0.0482 | train acc 98.3283 | validation acc 98.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-20): 100%|██████████| 20/20 [05:36<00:00, 16.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   19 | train loss 0.0512 | validation loss 0.0481 | train acc 98.4300 | validation acc 98.5500\n",
      "Evaluating pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> STAGE 2 (Pruned) COMPLETE ==\n",
      "    Accuracy: 98.55%\n",
      "\n",
      "===> Layerwise Prunning Results:\n",
      "    Layer name : conv2d_0, Original size 0.609375 Reduced size 0.609375:  Size Ratio: 0.00%\n",
      "    Layer name : conv2d_1, Original size 9.4375 Reduced size 4.12890625:  Size Ratio: 56.25%\n",
      "    Layer name : linear_0, Original size 131.578125 Reduced size 13.75:  Size Ratio: 89.55%\n",
      "    Layer name : linear_1, Original size 3.3203125 Reduced size 0.8203125:  Size Ratio: 75.29%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- STAGE 2: PRUNED ---\n",
    "# Use a copy to keep the original baseline model clean\n",
    "pruned_model = train_pruned(baseline_model, train_loader, test_loader)\n",
    "\n",
    "print(\"Evaluating pruned model...\")\n",
    "pruned_eval = pruned_model.evaluate(test_loader, {\"acc\": accuracy_fun}, device=DEVICE)\n",
    "print(f\"\\n==> STAGE 2 (Pruned) COMPLETE ==\")\n",
    "print(f\"    Accuracy: {pruned_eval['acc']:.2f}%\")\n",
    "\n",
    "print(f\"\\n===> Layerwise Prunning Results:\")\n",
    "for i, (name, module) in enumerate(pruned_model.names_layers()):\n",
    "    if (\"conv2d\" in name) or (\"linear\" in name):\n",
    "        print(f\"    Layer name : {name}, Original size {baseline_model[i].get_size_in_bits()/(8*1024)} Reduced size {module.get_size_in_bits()/(8*1024)}:  Size Ratio: {(1 - module.get_size_in_bits()/baseline_model[i].get_size_in_bits())*100:.2f}%\") # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b8dd1",
   "metadata": {},
   "source": [
    "### Quantized and Bit-Packed Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c594c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 3: Applying Quantization (QAT) & Retraining ---\n",
      "Applying quantization config: 4-bit, STATIC, PER_TENSOR\n",
      "Performing QAT (15 epochs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):   7%|▋         | 1/15 [00:35<08:16, 35.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0 | train loss 0.1089 | validation loss 0.0923 | train acc 96.3633 | validation acc 97.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  13%|█▎        | 2/15 [01:11<07:42, 35.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 | train loss 0.0980 | validation loss 0.0879 | train acc 96.6467 | validation acc 96.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  20%|██        | 3/15 [01:46<07:06, 35.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    2 | train loss 0.0962 | validation loss 0.0838 | train acc 96.6883 | validation acc 97.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  27%|██▋       | 4/15 [02:22<06:30, 35.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    3 | train loss 0.0948 | validation loss 0.0882 | train acc 96.7250 | validation acc 96.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  33%|███▎      | 5/15 [02:57<05:55, 35.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    4 | train loss 0.0980 | validation loss 0.0872 | train acc 96.6933 | validation acc 96.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  40%|████      | 6/15 [03:31<05:15, 35.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    5 | train loss 0.0924 | validation loss 0.0838 | train acc 96.8400 | validation acc 96.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  47%|████▋     | 7/15 [04:06<04:38, 34.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    6 | train loss 0.0922 | validation loss 0.0841 | train acc 96.8100 | validation acc 97.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  53%|█████▎    | 8/15 [04:41<04:04, 34.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    7 | train loss 0.0890 | validation loss 0.0798 | train acc 96.9867 | validation acc 97.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  60%|██████    | 9/15 [05:15<03:28, 34.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    8 | train loss 0.0897 | validation loss 0.0813 | train acc 96.9667 | validation acc 97.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  67%|██████▋   | 10/15 [05:50<02:53, 34.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    9 | train loss 0.0898 | validation loss 0.0796 | train acc 96.9250 | validation acc 97.1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  73%|███████▎  | 11/15 [06:23<02:17, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   10 | train loss 0.0919 | validation loss 0.0737 | train acc 96.8883 | validation acc 97.3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  80%|████████  | 12/15 [06:57<01:42, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   11 | train loss 0.0942 | validation loss 0.0738 | train acc 96.8867 | validation acc 97.4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  87%|████████▋ | 13/15 [07:31<01:08, 34.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   12 | train loss 0.0933 | validation loss 0.0817 | train acc 96.8633 | validation acc 97.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15):  93%|█████████▎| 14/15 [08:05<00:34, 34.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   13 | train loss 0.0904 | validation loss 0.0779 | train acc 96.9100 | validation acc 97.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DMC Training (Epochs 1-15): 100%|██████████| 15/15 [08:40<00:00, 34.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   14 | train loss 0.0909 | validation loss 0.0772 | train acc 96.8950 | validation acc 97.4600\n",
      "Evaluating final quantized-pruned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> STAGE 3 (Quantized-Pruned) COMPLETE ==\n",
      "    Accuracy: 97.59%\n",
      "\n",
      "===> Layerwise Pruned and Quantized Results:\n",
      "    Layer name : conv2d_0, Original size   0.6094 Reduced size   0.1064:  Size Ratio: 82.53%\n",
      "    Layer name : conv2d_1, Original size   9.4375 Reduced size   0.5498:  Size Ratio: 94.17%\n",
      "    Layer name : linear_0, Original size 131.5781 Reduced size   1.7969:  Size Ratio: 98.63%\n",
      "    Layer name : linear_1, Original size   3.3203 Reduced size   0.1465:  Size Ratio: 95.59%\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 3: QUANTIZED-PRUNED ---\n",
    "quantized_model = train_quantized_pruned(pruned_model, train_loader, test_loader)\n",
    "\n",
    "print(\"Evaluating final quantized-pruned model...\")\n",
    "quantized_eval = quantized_model.evaluate(test_loader, {\"acc\": accuracy_fun}, device=DEVICE)\n",
    "print(f\"\\n==> STAGE 3 (Quantized-Pruned) COMPLETE ==\")\n",
    "print(f\"    Accuracy: {quantized_eval['acc']:.2f}%\")\n",
    "\n",
    "print(f\"\\n===> Layerwise Pruned and Quantized Results:\")\n",
    "for i, (name, module) in enumerate(quantized_model.names_layers()):\n",
    "    if (\"conv2d\" in name) or (\"linear\" in name):\n",
    "        print(f\"    Layer name : {name}, Original size {baseline_model[i].get_size_in_bits()/(8*1024):8.4f} Reduced size {module.get_size_in_bits()/(8*1024):8.4f}:  Size Ratio: {(1 - module.get_size_in_bits()/baseline_model[i].get_size_in_bits())*100:.2f}%\") # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75cdfe5",
   "metadata": {},
   "source": [
    "### Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfcbd061",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_size = baseline_model.fuse(device=DEVICE).get_size_in_bytes()\n",
    "pruned_size = pruned_model.fuse(device=DEVICE).get_size_in_bytes()\n",
    "quantized_size = quantized_model.fuse(device=DEVICE).get_size_in_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca4be826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- REPRODUCTION FINISHED ---\n",
      "\n",
      "Final Results Summary:\n",
      "Baseline:   99.35% Acc,  144.95KB\n",
      "Pruned:     98.55% Acc,   19.31KB, 13.32% of original\n",
      "Quantized:  97.59% Acc,    2.60KB, 1.79% of original\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- REPRODUCTION FINISHED ---\")\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "print(f\"Baseline:   {baseline_eval['acc']:.2f}% Acc, {original_size/1024:7.2f}KB\")\n",
    "print(f\"Pruned:     {pruned_eval['acc']:.2f}% Acc, {pruned_size/1024:7.2f}KB, {pruned_size/original_size*100:.2f}% of original\")\n",
    "print(f\"Quantized:  {quantized_eval['acc']:.2f}% Acc, {quantized_size/1024:7.2f}KB, {quantized_size/original_size*100:.2f}% of original\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
