{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a4c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "from typing import Tuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import PIL \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5339369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/Documents/EmbeddedAI/deep-microcompression/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# sys.path.append(\"/home/matthias/Documents/EmbeddedAI/deep-microcompression/\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from development import (\n",
    "    Sequential,\n",
    "    AvgPool2d,\n",
    "    BatchNorm2d,\n",
    "    Conv2d,\n",
    "    Flatten,\n",
    "    Linear,\n",
    "    ReLU6,\n",
    "\n",
    "    EarlyStopper,\n",
    "\n",
    "    QuantizationGranularity,\n",
    "    QuantizationScheme\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43913a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mobilenetv1_file = f\"mobilenetv1_state_dict_from_tf_{DEVICE}.pth\"\n",
    "mobilenetv1_state_dict_dmc_original_from_tf = f\"mobilenetv1_state_dict_dmc_original_from_tf.pth\"\n",
    "\n",
    "input_shape = (3, 224, 224)\n",
    "LUCKY_NUMBER = 25\n",
    "torch.manual_seed(LUCKY_NUMBER)\n",
    "torch.random.manual_seed(LUCKY_NUMBER)\n",
    "torch.cuda.manual_seed(LUCKY_NUMBER)\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54f7db",
   "metadata": {},
   "source": [
    "## Getting the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09a8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageNet_Validation_DataSet(data.Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, combined_label_file, transformer=None):\n",
    "        \n",
    "        assert os.path.exists(image_dir), f\"image_dir {image_dir} doesn't exist\"\n",
    "        assert os.path.exists(combined_label_file), f\"combined_label_file {combined_label_file} doesn't exist!\"\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.images =  os.listdir(image_dir)\n",
    "        self.images.sort()\n",
    "\n",
    "        with open(combined_label_file, \"r\") as file:\n",
    "            self.labels, self.class_names = list(), list()\n",
    "            for line in file.readlines()[1:]:\n",
    "                _, _, tf_label, class_name = line.strip().split(\", \")\n",
    "                self.labels.append(tf_label)\n",
    "                self.class_names.append(class_name)\n",
    "\n",
    "        assert len(self.images) == len(self.labels), \"Images not of the same length as targets\"\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = PIL.Image.open(os.path.join(self.image_dir, self.images[idx])).convert(\"RGB\")\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "        return  image, int(self.labels[idx])\n",
    "        \n",
    "imagenet_transformer = transforms.Compose([\n",
    "    transforms.Resize(input_shape[1:]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.50], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "imagenet_val_dir = \"../../../Datasets/ImageNet_2012/validation/ILSVRC2012_img_val/\"\n",
    "imagenet_val_combined_label_file = \"../../../Datasets/ImageNet_2012/validation/ILSVRC2012_validation_combined_ground_truth.txt\"\n",
    "imagenet_val_dataset = ImageNet_Validation_DataSet(image_dir=imagenet_val_dir, combined_label_file=imagenet_val_combined_label_file, transformer=imagenet_transformer)\n",
    "imagenet_val_dataloader = data.DataLoader(imagenet_val_dataset, shuffle=False, batch_size=32, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f01e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    # transforms.RandomCrop((24, 24)),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "cifar10_train_dataset = datasets.CIFAR10(\"../../../Datasets/CIFAR_10/\", train=True, download=True, transform=data_transform)\n",
    "cifar10_test_dataset = datasets.CIFAR10(\"../../../Datasets/CIFAR_10/\", train=False, download=True, transform=data_transform)\n",
    "\n",
    "cifar10_train_loader = data.DataLoader(cifar10_train_dataset, batch_size=32, shuffle=True)\n",
    "cifar10_test_loader = data.DataLoader(cifar10_test_dataset, batch_size=32)\n",
    "\n",
    "cifar100_train_dataset = datasets.CIFAR100(\"../../../Datasets/CIFAR_100/\", train=True, download=True, transform=data_transform)\n",
    "cifar100_test_dataset = datasets.CIFAR100(\"../../../Datasets/CIFAR_100/\", train=False, download=True, transform=data_transform)\n",
    "\n",
    "cifar100_train_loader = data.DataLoader(cifar100_train_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count())\n",
    "cifar100_test_loader = data.DataLoader(cifar100_test_dataset, batch_size=32, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d44077",
   "metadata": {},
   "source": [
    "## Building the model and Loading the Saved Weights\n",
    "## Defining the Mobilenetv1 Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe61828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MobileNetV1(load_tf_weights=True):\n",
    "\n",
    "    def ConvBatchReLU(\n",
    "            in_channels:int,\n",
    "            out_channels:int,\n",
    "            kernel_size:int,\n",
    "            stride:int = 1,\n",
    "            groups:int = 1,\n",
    "            pad:Tuple[int, int, int, int] = (0, 0, 0, 0),\n",
    "            bias=False,\n",
    "            eps=0.001, \n",
    "            momentum=0.01,\n",
    "    ):\n",
    "        return (Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, groups=groups, pad=pad, bias=bias),\n",
    "                BatchNorm2d(num_features=out_channels, eps=eps, momentum=momentum, affine=True, track_running_stats=True,),\n",
    "                ReLU6(inplace=True))\n",
    "    \n",
    "    def DepthwiseSeperableConv2LUBatchReLU(\n",
    "        in_channels:int,\n",
    "        out_channels:int,\n",
    "        kernel_size:int,\n",
    "        stride:int,\n",
    "        pad:Tuple[int, int, int, int] = (0, 0, 0, 0),\n",
    "        eps=0.001, \n",
    "        momentum=0.01\n",
    "    ):\n",
    "        return (\n",
    "            *ConvBatchReLU(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=stride, pad=pad, groups=in_channels, eps=eps, momentum=momentum),\n",
    "            *ConvBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, pad=(0,0,0,0), groups=1, eps=eps, momentum=momentum)\n",
    "        )\n",
    "    mobilenetv1 = {\n",
    "        \"conv2d_0\": [3, 32, 3, 2],\n",
    "\n",
    "        \"depthwiseseparable_0\": [32, 64, 3, 1],\n",
    "        \"depthwiseseparable_1\": [64, 128, 3, 2],\n",
    "        \"depthwiseseparable_2\": [128, 128, 3, 1],\n",
    "        \"depthwiseseparable_3\": [128, 256, 3, 2],\n",
    "        \"depthwiseseparable_4\": [256, 256, 3, 1],\n",
    "        \"depthwiseseparable_5\": [256, 512, 3, 2],\n",
    "\n",
    "        \"depthwiseseparable_6\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_7\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_8\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_9\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_10\": [512, 512, 3, 1],\n",
    "\n",
    "        \"depthwiseseparable_11\": [512, 1024, 3, 2],\n",
    "        \"depthwiseseparable_12\": [1024, 1024, 3, 1],\n",
    "\n",
    "        \"avgpool_0\": [7],\n",
    "        \"conv2d_1\": [1024, 1000, 1, 1],\n",
    "        \"flatten_0\": []\n",
    "    }\n",
    "\n",
    "    batchnorm_eps = 0.001\n",
    "    batchnorm_momentum = 1 - 0.99\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    for name, parameters in mobilenetv1.items():\n",
    "        if \"conv2d_0\" in name:\n",
    "            in_channels, out_channels, kernel_size, stride = parameters\n",
    "            if stride == 2:\n",
    "                pad = (0, 1, 0, 1)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected type Conv layer\")\n",
    "            layers.extend(ConvBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, pad=pad, eps=batchnorm_eps, momentum=batchnorm_momentum))\n",
    "        elif \"conv2d_1\" in name:\n",
    "            in_channels, out_channels, kernel_size, stride = parameters\n",
    "            layers.append(Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride))\n",
    "        elif \"depthwiseseparable\" in name:\n",
    "            in_channels, out_channels, kernel_size, stride = parameters\n",
    "            if stride == 2:\n",
    "                pad = (0, 1, 0, 1)\n",
    "            else:\n",
    "                pad = tuple([1]*4)\n",
    "            layers.extend(DepthwiseSeperableConv2LUBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, pad=pad, eps=batchnorm_eps, momentum=batchnorm_momentum))\n",
    "        elif \"avgpool\" in name:\n",
    "            kernel_size = parameters[0]\n",
    "            layers.append(AvgPool2d(kernel_size=kernel_size))\n",
    "        elif \"flatten\" in name:\n",
    "            layers.append(Flatten())\n",
    "        else:\n",
    "            raise ValueError(f\"Recieved unexpected layer of {name}\")\n",
    "    mobilenetv1_model = Sequential(*layers)\n",
    "    if load_tf_weights:\n",
    "        mobilenetv1_model.load_state_dict(torch.load(mobilenetv1_state_dict_dmc_original_from_tf, weights_only=True), strict=True)\n",
    "    return mobilenetv1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ef2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_acc_fun = lambda y_pred, y_true: ((y_pred).argmax(dim=1) == y_true).sum().item()*100\n",
    "top5_acc_fun = lambda y_pred, y_true: (y_pred.topk(5, dim=1).indices == y_true.unsqueeze(1)).any(dim=1).sum().item()*100\n",
    "\n",
    "metrics = {\n",
    "    \"top1acc\": top1_acc_fun,\n",
    "    \"top5acc\" : top5_acc_fun\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f791d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [05:35<00:00,  4.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16927904, {'top1acc': 69.042, 'top5acc': 88.508})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenetv1_model = MobileNetV1()\n",
    "mobilenetv1_model.to(DEVICE)\n",
    "\n",
    "original_results = mobilenetv1_model.evaluate(imagenet_val_dataloader, metrics, DEVICE)\n",
    "original_size = mobilenetv1_model.get_size_in_bytes()\n",
    "original_size, original_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv1_model.get_layers_prune_channel_sensity(input_shape, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3e402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenetv1_model(next(iter(cifar100_test_loader))[0].to(DEVICE)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aea7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 675/1563 [02:09<02:49,  5.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m mobilenetv1_model_fused = MobileNetV1().fuse()\n\u001b[32m      2\u001b[39m mobilenetv1_model_fused.to(DEVICE)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m fused_results = \u001b[43mmobilenetv1_model_fused\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagenet_val_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtop1_acc_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtop5_acc_fun\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m fused_size = mobilenetv1_model_fused.get_size_in_bytes()\n\u001b[32m      6\u001b[39m fused_size, fused_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EmbeddedAI/deep-microcompression/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EmbeddedAI/deep-microcompression/tests/mobilenetv1/../../development/models/sequential.py:298\u001b[39m, in \u001b[36mSequential.evaluate\u001b[39m\u001b[34m(self, data_loader, metrics, device)\u001b[39m\n\u001b[32m    296\u001b[39m         y_pred = \u001b[38;5;28mself\u001b[39m(X)\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m metric_name, metric_func \u001b[38;5;129;01min\u001b[39;00m metrics.items():\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m             metric_results[metric_name] += \u001b[43mmetric_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m###############################################\u001b[39;00m\n\u001b[32m    300\u001b[39m         \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m###############################################\u001b[39;00m\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m metric_name \u001b[38;5;129;01min\u001b[39;00m metrics.keys():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(y_pred, y_true)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m top1_acc_fun = \u001b[38;5;28;01mlambda\u001b[39;00m y_pred, y_true: \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m*\u001b[32m100\u001b[39m\n\u001b[32m      2\u001b[39m top5_acc_fun = \u001b[38;5;28;01mlambda\u001b[39;00m y_pred, y_true: (y_pred.topk(\u001b[32m5\u001b[39m, dim=\u001b[32m1\u001b[39m).indices == y_true.unsqueeze(\u001b[32m1\u001b[39m)).any(dim=\u001b[32m1\u001b[39m).sum().item()*\u001b[32m100\u001b[39m\n\u001b[32m      4\u001b[39m metrics = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtop1acc\u001b[39m\u001b[33m\"\u001b[39m: top1_acc_fun,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtop5acc\u001b[39m\u001b[33m\"\u001b[39m : top5_acc_fun\n\u001b[32m      7\u001b[39m }\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "mobilenetv1_model_fused = MobileNetV1().fuse()\n",
    "mobilenetv1_model_fused.to(DEVICE)\n",
    "\n",
    "fused_results = mobilenetv1_model_fused.evaluate(imagenet_val_dataloader, {\"top1\":top1_acc_fun, \"top5\":top5_acc_fun}, DEVICE)\n",
    "fused_size = mobilenetv1_model_fused.get_size_in_bytes()\n",
    "fused_size, fused_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b3a89",
   "metadata": {},
   "source": [
    "## Testing Compression on MobileNetV1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee677e",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f7357",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m compression_config = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprune_channel\u001b[39m\u001b[33m\"\u001b[39m :{\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msparsity\u001b[39m\u001b[33m\"\u001b[39m : sp,\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetric\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33ml2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     },\n\u001b[32m      8\u001b[39m }\n\u001b[32m      9\u001b[39m compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m evaluate_result = \u001b[43mcompressed_mobilenetv1_mcu_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagenet_val_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop1_acc_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m*\u001b[32m100\u001b[39m\n\u001b[32m     12\u001b[39m size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBefore training, sparsity = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc1 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore_acc1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc5 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore_acc5\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize/original_size*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m9.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EmbeddedAI/deep-microcompression/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EmbeddedAI/deep-microcompression/tests/mobilenetv1/../../development/models/sequential.py:287\u001b[39m, in \u001b[36mSequential.evaluate\u001b[39m\u001b[34m(self, data_loader, metrics, device)\u001b[39m\n\u001b[32m    285\u001b[39m metric_results = \u001b[38;5;28mdict\u001b[39m()\n\u001b[32m    286\u001b[39m data_len = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m():\n\u001b[32m    288\u001b[39m     metric_results[metric_name] = \u001b[32m0\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28mself\u001b[39m.eval()\n",
      "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "for i in range(0, 11, 1):\n",
    "    sp = i/10\n",
    "    compression_config = {\n",
    "        \"prune_channel\" :{\n",
    "            \"sparsity\" : sp,\n",
    "            \"metric\" : \"l2\"\n",
    "        },\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape)\n",
    "\n",
    "    evaluate_result = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, metrics, device=DEVICE)*100\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, sparsity = {sp} acc1 = {before_acc1:.4f} acc5 = {before_acc5:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb731f",
   "metadata": {},
   "source": [
    "### Dynamic Quantization Per Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48377c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in [8, 4, 2]:\n",
    "    compression_config = {\n",
    "        \"quantize\" : {\n",
    "            \"scheme\" : QuantizationScheme.DYNAMIC,\n",
    "            \"granularity\": QuantizationGranularity.PER_TENSOR,\n",
    "            \"bitwidth\" : b\n",
    "        }\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape)\n",
    "\n",
    "    before_acc1 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top1_acc_fun, device=DEVICE)*100\n",
    "    before_acc5 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top5_acc_fun, device=DEVICE)*100\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, bitwidth = {b} acc1 = {before_acc1:.4f} acc5 = {before_acc5:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8bc10",
   "metadata": {},
   "source": [
    "### Dynamic Quantization Per Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in [8, 4, 2]:\n",
    "    compression_config = {\n",
    "        \"quantize\" : {\n",
    "            \"scheme\" : QuantizationScheme.DYNAMIC,\n",
    "            \"granularity\": QuantizationGranularity.PER_CHANNEL,\n",
    "            \"bitwidth\" : b\n",
    "        }\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape)\n",
    "\n",
    "    before_acc1 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top1_acc_fun, device=DEVICE)*100\n",
    "    before_acc5 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top5_acc_fun, device=DEVICE)*100\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, bitwidth = {b} acc1 = {before_acc1:.4f} acc5 = {before_acc5:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3df2bc",
   "metadata": {},
   "source": [
    "### Static Quantization Per Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [19:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, bitwidth = 8 acc1 = 2.5440 acc5 = 7.9060 size =   25.2139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [21:22<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, bitwidth = 4 acc1 = 0.1000 acc5 = 0.5000 size =   12.7493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [19:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, bitwidth = 2 acc1 = 0.1000 acc5 = 0.5000 size =    6.5169\n"
     ]
    }
   ],
   "source": [
    "for b in [8, 4, 2]:\n",
    "    compression_config = {\n",
    "        \"quantize\" : {\n",
    "            \"scheme\" : QuantizationScheme.STATIC,\n",
    "            \"granularity\": QuantizationGranularity.PER_TENSOR,\n",
    "            \"bitwidth\" : b\n",
    "        }\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.fuse().to(DEVICE).init_compress(compression_config, input_shape=input_shape, calibration_data=next(iter(imagenet_val_dataloader))[0].to(DEVICE))\n",
    "\n",
    "    metrics_result = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, metrics, device=DEVICE)\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, bitwidth = {b} acc1 = {metrics_result[\"top1acc\"]:.4f} acc5 = {metrics_result[\"top5acc\"]:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b9d4c",
   "metadata": {},
   "source": [
    "### Static Quantization Per Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [18:44<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, bitwidth = 8 acc1 = 67.3800 acc5 = 87.4620 size =   25.4962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [31:30<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, bitwidth = 4 acc1 = 0.1700 acc5 = 0.8720 size =   13.0316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 255/1563 [06:31<31:49,  1.46s/it]"
     ]
    }
   ],
   "source": [
    "for b in [8, 4, 2]:\n",
    "    compression_config = {\n",
    "        \"quantize\" : {\n",
    "            \"scheme\" : QuantizationScheme.STATIC,\n",
    "            \"granularity\": QuantizationGranularity.PER_CHANNEL,\n",
    "            \"bitwidth\" : b\n",
    "        }\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.fuse().to(DEVICE).init_compress(compression_config, input_shape=input_shape, calibration_data=next(iter(imagenet_val_dataloader))[0].to(DEVICE))\n",
    "\n",
    "    metrics_result = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, metrics, device=DEVICE)\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, bitwidth = {b} acc1 = {metrics_result[\"top1acc\"]:.4f} acc5 = {metrics_result[\"top5acc\"]:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944c78d",
   "metadata": {},
   "source": [
    "## Training with compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b1ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "sp = 1/10\n",
    "compression_config = {\n",
    "    \"prune_channel\" :{\n",
    "        \"sparsity\" : sp,\n",
    "        \"metric\" : \"l2\"\n",
    "    },\n",
    "}\n",
    "compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape)\n",
    "\n",
    "# metrics_result = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, metrics, device=DEVICE)\n",
    "# size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "# print(f\"Before training, sparsity = {sp} acc1 = {metrics_result[\"top1acc\"]:.4f} acc5 = {metrics_result[\"top5acc\"]:.4f} size = {size/original_size*100:9.4f}\")\n",
    "\n",
    "\n",
    "criterion_func = nn.CrossEntropyLoss()\n",
    "optimization_func = optim.Adam(compressed_mobilenetv1_mcu_model.parameters(), lr=1.e-3)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimization_func, mode=\"min\", patience=2)\n",
    "\n",
    "compressed_mobilenetv1_mcu_model.fit(\n",
    "    imagenet_val_dataloader, 5, \n",
    "    criterion_func, optimization_func, lr_scheduler,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE\n",
    ")\n",
    "torch.save(compressed_mobilenetv1_mcu_model, \"compressed_mobilenetv1_mcu_model\")\n",
    "# metrics_result = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, metrics, device=DEVICE)\n",
    "# print(f\"After training, sparsity = {sp} acc1 = {metrics_result[\"top1acc\"]:.4f} acc5 = {metrics_result[\"top5acc\"]:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12415ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top1acc': 0.216, 'top5acc': 0.896}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030b069",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m compressed_mobilenetv1_mcu_model = \u001b[43mtorch\u001b[49m.load(\u001b[33m\"\u001b[39m\u001b[33mcompressed_mobilenetv1_mcu_model\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "compressed_mobilenetv1_mcu_model = torch.load(\"compressed_mobilenetv1_mcu_model\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801380e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m criterion_func = \u001b[43mnn\u001b[49m.CrossEntropyLoss()\n\u001b[32m      2\u001b[39m optimization_func = optim.Adam(compressed_mobilenetv1_mcu_model.parameters(), lr=\u001b[32m1.e-3\u001b[39m)\n\u001b[32m      3\u001b[39m lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimization_func, mode=\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m, patience=\u001b[32m2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "criterion_func = nn.CrossEntropyLoss()\n",
    "optimization_func = optim.Adam(compressed_mobilenetv1_mcu_model.parameters(), lr=1.e-3)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimization_func, mode=\"min\", patience=2)\n",
    "\n",
    "compressed_mobilenetv1_mcu_model.fit(\n",
    "    imagenet_val_dataloader, 1, \n",
    "    criterion_func, optimization_func, lr_scheduler,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE\n",
    ")\n",
    "torch.save(compressed_mobilenetv1_mcu_model, \"compressed_mobilenetv1_mcu_model\")\n",
    "# metrics_result = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, metrics, device=DEVICE)\n",
    "# print(f\"After training, sparsity = {sp} acc1 = {metrics_result[\"top1acc\"]:.4f} acc5 = {metrics_result[\"top5acc\"]:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3238f2",
   "metadata": {},
   "source": [
    "# MobileNetV1 Tensorflow to Torch Converter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98babdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def convert_mobilenetv1_tf_to_torch(save_weight=True):\n",
    "\n",
    "    def copy_tensor(tensor_source, tensor_destination):\n",
    "        tensor_destination.copy_(tensor_source)\n",
    "\n",
    "    mobilenetv1_tf_model = tf.keras.applications.MobileNet(weights=\"imagenet\")\n",
    "    torch_layers = []\n",
    "\n",
    "    for i, layer in enumerate(mobilenetv1_tf_model.layers):\n",
    "    \n",
    "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            # print(layer.weights[0].shape)\n",
    "\n",
    "            weight = np.transpose(layer.weights[0], (3, 2, 0, 1))\n",
    "            out_channels, in_channels, kernel_size, _ = weight.shape\n",
    "            stride =layer.strides[0]\n",
    "\n",
    "            if kernel_size == 3 and stride == 2:\n",
    "                pad = (0, 1, 0, 1)\n",
    "                padding = 0\n",
    "            elif kernel_size == 1 and stride == 1:\n",
    "                pad = (0,0,0,0)\n",
    "                padding = 0\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected type Conv layer\")\n",
    "\n",
    "            # torch_layers.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=layer.strides[0], padding=1 bias=True))\n",
    "            torch_layers.append(Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "            kernel_size=kernel_size, stride=stride, padding=padding, pad=pad, \n",
    "            bias=len(layer.weights)==2))\n",
    "            copy_tensor(torch.from_numpy(weight), torch_layers[-1].weight)\n",
    "                \n",
    "            if len(layer.weights) == 2:\n",
    "                # nn.init.constant_(torch_layers[-1].bias, 0.0)\n",
    "            # else:\n",
    "                copy_tensor(torch.from_numpy(layer.weights[1].numpy()), torch_layers[-1].bias)\n",
    "\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "            # Convert to PyTorch format: [in_channels, 1, H, W]\n",
    "            weight = np.transpose(layer.weights[0], (2, 3, 0, 1))\n",
    "            \n",
    "            in_channels, depth_multiplier, kernel_size, _ = weight.shape\n",
    "            stride =layer.strides[0]\n",
    "\n",
    "            assert depth_multiplier == 1, \"Depth multiplier must be 1 for depthwise convolutions\"\n",
    "\n",
    "            if kernel_size == 3 and stride == 2:\n",
    "                padding = 0\n",
    "                pad = (0, 1, 0, 1)\n",
    "            elif kernel_size == 3 and stride == 1:\n",
    "                padding = 0\n",
    "                pad = (1,1,1,1)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected type Conv layer\")\n",
    "\n",
    "            # torch_layers.append(nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=layer.strides[0], padding=1, groups=in_channels, bias=True))\n",
    "            torch_layers.append(\n",
    "        Conv2d(in_channels=in_channels, out_channels=in_channels, \n",
    "        kernel_size=kernel_size, stride=stride, padding=padding, pad=pad, \n",
    "        groups=in_channels, bias=len(layer.weights)==2))\n",
    "            print(i, weight.shape, torch_layers[-1].weight.shape, in_channels)\n",
    "\n",
    "            # Copy weights\n",
    "            copy_tensor(torch.from_numpy(weight), torch_layers[-1].weight)\n",
    "            # nn.init.constant_(torch_layers[-1].bias, 0.0)\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.ReLU):\n",
    "            # torch_layers.append(nn.ReLU())\n",
    "            torch_layers.append(ReLU6())\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            gamma, beta, mean, var = layer.weights\n",
    "            out_channels = gamma.shape[0]\n",
    "\n",
    "            # torch_layers.append(nn.BatchNorm2d(out_channels, eps=layer.epsilon, momentum=layer.momentum))\n",
    "            torch_layers.append(BatchNorm2d(out_channels, eps=layer.epsilon, momentum=1 - layer.momentum, affine=layer.center and layer.scale,  track_running_stats=True,))\n",
    "            # print(torch_layers[-1].momentum)  \n",
    "            # print(layer.epsilon, layer.momentum, layer.center, layer.scale)\n",
    "            copy_tensor(torch.from_numpy(gamma.numpy()), torch_layers[-1].weight)\n",
    "            copy_tensor(torch.from_numpy(beta.numpy()), torch_layers[-1].bias)\n",
    "            copy_tensor(torch.from_numpy(mean.numpy()), torch_layers[-1].running_mean)\n",
    "            copy_tensor(torch.from_numpy(var.numpy()), torch_layers[-1].running_var)\n",
    "\n",
    "            # print(torch_layers[-1].running_var, layer.weights)\n",
    "\n",
    "            # print(var)\n",
    "            # print(layer.moving_variance)\n",
    "\n",
    "            # break\n",
    "            pass\n",
    "            \n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            weight, bias = layer.weights\n",
    "            # torch_layers.append(nn.Linear(weight.shape[0], weight.shape[1]))\n",
    "            torch_layers.append(Linear(weight.shape[0], weight.shape[1]))\n",
    "\n",
    "            copy_tensor(torch.from_numpy(weight.numpy().T), torch_layers[-1].weight)\n",
    "            copy_tensor(torch.from_numpy(bias.numpy()), torch_layers[-1].bias)\n",
    "\n",
    "            # print(weight, bias)\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.GlobalAveragePooling2D):\n",
    "            torch_layers.append(AvgPool2d(kernel_size=7))\n",
    "            # torch_layers.append(MaxPool2d(kernel_size=7))\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.ZeroPadding2D):\n",
    "            pass\n",
    "        elif isinstance(layer, tf.keras.layers.Dropout):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Reshape):\n",
    "            # print(\"Not Needed\")\n",
    "            # torch_layers.append(Flatten())\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Activation):\n",
    "            # torch_layers.append(nn.Softmax(dim=1))\n",
    "            pass\n",
    "\n",
    "        else: \n",
    "            pass\n",
    "            raise RuntimeError(f\"Unknown layer type: {type(layer)}\")\n",
    "    mobilenetv1_model = Sequential(*torch_layers)\n",
    "    if save_weight:\n",
    "        torch.save(mobilenetv1_model.state_dict(), mobilenetv1_state_dict_dmc_original_from_tf)\n",
    "    return mobilenetv1_model\n",
    "    # print(layer.name, type(layer))\n",
    "mobilenetv1_torch_model = convert_mobilenetv1_tf_to_torch(save_weight=False)\n",
    "type(mobilenetv1_torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b47d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
